{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab Session : Feature extraction**\n",
    "\n",
    "Author: Vanessa GÃ³mez Verdejo (http://vanessa.webs.tsc.uc3m.es/)\n",
    "\n",
    "Updated: 27/02/2017 (working with sklearn 0.18.1)\n",
    "\n",
    "In this lab session we are going to work with some of the most well-known feature extraction techniques, three MVA approaches: PCA, PLS and CCA, and the Linear Discrmination Analysis (LDA).\n",
    "\n",
    "We will start working over a face detection problem where we will be able to test the performance improvements provided by the feature extraction process and, besides, from a qualitative point of view, analyze the extracted features by means of plotting the eigenvectors defining the feature extraction process. \n",
    "\n",
    "To analyze the discriminatory capability of the extracted features, let's use a linear SVM as classifier and analyze its final accuracy over the test data to evaluate the goodness of the different feature extraction methods.\n",
    "\n",
    "To implement the different approaches we will base on [Scikit-Learn](http://scikit-learn.org/stable/) python toolbox.\n",
    "\n",
    "#### ** During this lab we will cover: **\n",
    "#### *Part 1: Linear feature selection* \n",
    "##### *   Part 1.1: Unsupervised feature selection: PCA* \n",
    "##### *   Part 1.2: Supervised feature selection: PLS,  CCA & LDA* \n",
    "\n",
    "As you progress in this notebook, you will have to complete some exercises. Each exercise includes an explanation of what is expected, followed by code cells where one or several lines will have written down `<FILL IN>`.  The cell that needs to be modified will have `# TODO: Replace <FILL IN> with appropriate code` on its first line.  Once the `<FILL IN>` sections are updated and the code can be run; below this cell, you will find the test cell (beginning with the line `# TEST CELL`) and you can run it to verify the correctness of your solution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 0: Download and prepare the data **\n",
    "\n",
    "The dataset consists of ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).\n",
    "    \n",
    "The next code includes the lines to download this data set and create the training, validation and test data partitions, as well as normalize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_olivetti_faces \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "print 'The first time that you download the data it can take a while...'\n",
    "olivetti_people = fetch_olivetti_faces()\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = olivetti_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = olivetti_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "Y = olivetti_people.target\n",
    "n_classes = np.unique(Y).shape[0]\n",
    "\n",
    "print(\"Dataset size information:\")\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Preparing the data\n",
    "\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# split into a training and validation set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.333)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Binarize the labels for supervised feature extraction methods\n",
    "set_classes = np.unique(Y)\n",
    "Y_train_bin = label_binarize(Y_train, classes=set_classes)\n",
    "\n",
    "print(\"Number of training samples: %d\" % X_train.shape[0])\n",
    "print(\"Number of validation samples: %d\" % X_val.shape[0])\n",
    "print(\"Number of test samples: %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Representing some example faces **\n",
    "\n",
    "Next cells include the plot_gallery( ) function, which let you plot any of the face images from the data set and in later sections plot the eigenvectors provided by the feature selection process. Besides, as an example, the function is used to plot some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_gallery(images, titles, h, w, n_row=4, n_col=10):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As example, we plot a face of each class (or person)\n",
    "titles = ['class '+str(c) for c in set_classes]\n",
    "ind_faces = [np.where(Y_train == c)[0][0] for c in set_classes]\n",
    "\n",
    "plot_gallery(X_train[ind_faces,:], titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Part 1: Linear feature selection* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1.1: Unsupervised feature selection: PCA **\n",
    "\n",
    "Here, let's use the [PCA( )](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) method to find the projections maximizing the variance of the projected data. Complete next cell to obtain:\n",
    "* The first 100 projection vectors from the training data.\n",
    "* The projections of training, validation and test data in this new space.\n",
    "* The dimensions (number of data and number of features) of the projected data.\n",
    "\n",
    "Use the methods .fit( ), fit_transform( ) and transform ( ) of the pca object to solve the exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "N_feat_max=100\n",
    "# Define and train pca object\n",
    "pca = PCA(n_components=N_feat_max)\n",
    "pca.#<FILL IN>\n",
    "\n",
    "# Project the training, validation and test data\n",
    "X_train_pca = #<FILL IN>\n",
    "X_test_pca = #<FILL IN>\n",
    "X_val_pca = #<FILL IN>\n",
    "\n",
    "# Compute new dimensions\n",
    "dim_train = #<FILL IN>\n",
    "dim_val = #<FILL IN>\n",
    "dim_test = #<FILL IN>\n",
    "\n",
    "print 'Dimensions of training data are: ' + str(dim_train)\n",
    "print 'Dimensions of validation data are: ' + str(dim_val)\n",
    "print 'Dimensions of test data are: ' + str(dim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(dim_train, (200, 100), 'incorrect result: training data dimensions are uncorrect')\n",
    "Test.assertEquals(dim_val, (100, 100), 'incorrect result: validation data dimensions are uncorrect')\n",
    "Test.assertEquals(dim_test, (100, 100), 'incorrect result: test data dimensions are uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Analyzing eigenvectors **\n",
    "\n",
    "Execute the following code to plot the first eigenvectors. The analysis of eigenvectors in face detection problems is quite common, in fact, they are known as eigenfaces. Note that to construct each new feature, each input face is projected over the eigenfaces and a new value (new feature) is obtained for each eigenvector. Lighter regions of the eigenfaces will contribute positively to generate the new feature, whereas darkness regions will contribute negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot eigenfaces\n",
    "n_eigenfaces=20\n",
    "titles = ['Eigenface '+str(num) for num in range(n_eigenfaces)]\n",
    "eigenfaces = pca.components_.reshape((N_feat_max, h, w))\n",
    "plot_gallery(eigenfaces[:n_eigenfaces,:,:], titles, h, w, n_row=2, n_col=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Evaluating performance **\n",
    "\n",
    "Here let's use a linear SVM to evaluate the discriminatory capability of the extracted features. Complete next code to design the function SVM_accuracy_evolution( ) which has to be able to compute the training, validation and test accuracies of a linear SVM (with the default penalty parameter) for the number of extracted features given in the parameter 'rang\\_feat'.\n",
    "\n",
    "Last code lines let you run this function with the PCA projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def SVM_accuracy_evolution(X_train_t, Y_train, X_val_t, Y_val, X_test_t, Y_test, rang_feat):\n",
    "    \"\"\"Compute the accuracy of training, validation and test data for different the number of features given\n",
    "        in rang_feat.\n",
    "\n",
    "    Args:\n",
    "        X_train_t (numpy dnarray): training data projected in the new feature space (number data x number dimensions).\n",
    "        Y_train (numpy dnarray): labels of the training data (number data x 1).\n",
    "        X_val_t (numpy dnarray): validation data projected in the new feature space (number data x number dimensions).\n",
    "        Y_val (numpy dnarray): labels of the validation data (number data x 1).\n",
    "        X_test_t (numpy dnarray): test data projected in the new feature space (number data x number dimensions).\n",
    "        Y_test (numpy dnarray): labels of the test data (number data x 1).\n",
    "        rang_feat: range with different number of features to be evaluated                                           \n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the model to train a liner SVM\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    acc_tr = []\n",
    "    acc_val = []\n",
    "    acc_test = []\n",
    "    for i in rang_feat:\n",
    "        # Train SVM classifier\n",
    "        #<FILL IN>\n",
    "        \n",
    "        # Compute accuracies\n",
    "        acc_tr.append(#<FILL IN>)\n",
    "        acc_val.append(#<FILL IN>)\n",
    "        acc_test.append(#<FILL IN>)\n",
    "\n",
    "    return np.array(acc_tr), np.array(acc_val), np.array(acc_test)\n",
    "                    \n",
    "# Run the function with the pca extracted features                    \n",
    "rang_feat = np.arange(5, N_feat_max, 10) # To speed up the execution, we use steps of 10 features.\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(X_train_pca, Y_train, X_val_pca, Y_val, X_test_pca, Y_test, rang_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make easier evaluate the above computed accuracies, next code let you plot the train, validation and test accuracies. Have a look to function plot\\_accuracy\\_evolution( ) which will be used in the following sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test):\n",
    "    \"\"\"Plot the accuracy evolution for training, validation and test data sets.\n",
    "\n",
    "    Args:\n",
    "        rang_feat: range with different number of features where the accuracy has been evaluated   \n",
    "        acc_tr: numpy vector with the training accuracies\n",
    "        acc_val: numpy vector with the validation accuracies\n",
    "        acc_test: numpy vector with the test accuracies                                          \n",
    "   \n",
    "    \"\"\"\n",
    "    plt.plot(rang_feat, acc_tr, \"b\", label=\"train\")\n",
    "    plt.plot(rang_feat, acc_val, \"g\", label=\"validation\")\n",
    "    plt.plot(rang_feat, acc_test, \"r\", label=\"test\")\n",
    "    plt.xlabel(\"Number of features\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title('Accuracy evolution')\n",
    "    plt.legend(['Training', 'Validation', 'Test'], loc = 4)\n",
    "    \n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Select the optimum number of features **\n",
    "\n",
    "Use the validation accuracy vector to obtain optimum number of features and provide the test error for this number of features. Function argmax( ) of numpy library may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Find the optimum number of features\n",
    "pos_max = #<FILL IN>\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat, 35, 'incorrect result: number of optimum dimensions is uncorrect')\n",
    "Test.assertEquals(np.round(test_acc_opt,4), 0.8800, 'incorrect result: test accuracy is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1.2: Supervised feature extraction: PLS, CCA & LDA **\n",
    "\n",
    "In this section, we are going to analyze the advantages of using supervised feature extraction techniques, such as, Partial Least Squares (PLS), Canonical Correlaton Analisys (CCA) and Linear Discrimination Analysis (LDA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Partial Least Squares (PLS) **\n",
    "\n",
    "PLS aims at finding the projections of the input and output data with maximum covariance, so it includes the label information to obtain the new data representation.\n",
    "\n",
    "Start computing the PLS approach with the method [PLSSVD( )](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html) and obtain:\n",
    "1. The first eigenvectors from the training data and obtain the projections of training, validation and test data. Note that in this case you can only obtain as many new projections as number of categories.\n",
    "3. Compute the SVM accuracy provided by different number of extracted features and plot it. You can use the SVM_accuracy_evolution( ) and plot_accuracy_evolution( ) functions of the previous section.\n",
    "4. Obtain the optimum number of projected features and its corresponding test accuracy.\n",
    "\n",
    "Note: to work with the supervised feature extraction methods, you have to use the binarized label vector (Y_train_bin); whereas, to train the linear SVM you can go on using the standard label codification (Y_train, Y_val and Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn.cross_decomposition import PLSSVD\n",
    "\n",
    "N_feat_max = n_classes -1 # As many new features as classes minus 1\n",
    "# 1. Obtain PLS projections\n",
    "pls = PLSSVD(n_components=N_feat_max)\n",
    "pls.#<FILL IN>\n",
    "X_train_pls = #<FILL IN>\n",
    "X_val_pls = #<FILL IN>\n",
    "X_test_pls = #<FILL IN>\n",
    "\n",
    "# 2. Compute and plot accuracy evolution\n",
    "rang_feat = np.arange(1, N_feat_max, 1) \n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(X_train_pls, Y_train, X_val_pls, Y_val, X_test_pls, Y_test, rang_feat)\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# 3. Find the optimum number of features\n",
    "pos_max = #<FILL IN>\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat, 15, 'incorrect result: number of optimum dimensions is uncorrect')\n",
    "Test.assertEquals(np.round(test_acc_opt,4), 0.8200, 'incorrect result: test accuracy is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Canonical Correlation Analysis (CCA) **\n",
    "\n",
    "CCA algorithm is also a supervised MVA approach,  since it searches for the directions of maximum correlation between input and output data.\n",
    "\n",
    "In the next cell, let's repeat the previous steps to obtain the accuracy evolution with the number of features, but using the CCA algorithm.\n",
    "\n",
    "To implement it, one could use the method [CCA()](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html); however, this implementation of Scikit-Learn presents some convergence problems when more than just a few eigenvectors are computed. So, you can use the CCA implementation that is included in the library mva.py (you can find it in the same repository than this notebook); the first lines of the following cell explain how to call and use this CCA implementation.  \n",
    "\n",
    "Note: this implementation includes a regularization penalty. For this experiment, set its value to 1e-2; however, take into account that you should crossvalidate it for a real application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from lib.mva import mva\n",
    "N_feat_max = n_classes # As many new features as classes \n",
    "# 1. Obtain CCA projections\n",
    "CCA = mva ('CCA', N_feat_max)\n",
    "CCA.#<FILL IN>  # Here, set reg = 1e-2\n",
    "X_train_cca = #<FILL IN>\n",
    "X_val_cca = #<FILL IN>\n",
    "X_test_cca = #<FILL IN>\n",
    "\n",
    "# 2. Compute and plot accuracy evolution\n",
    "rang_feat = np.arange(1, N_feat_max, 1)\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(X_train_cca, Y_train, X_val_cca, Y_val, X_test_cca, Y_test, rang_feat)\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# 3. Find the optimum number of features\n",
    "pos_max = #<FILL IN>\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat, 31, 'incorrect result: number of optimum dimensions is uncorrect')\n",
    "Test.assertEquals(np.round(test_acc_opt,4), 0.9100, 'incorrect result: test accuracy is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Linear Discrimination Analysis (LDA) **\n",
    "\n",
    "The last feature extraction method to study in this notebook is the Linear Discrimination Analysis (LDA). LDA is known as a generative classifier based on considering data are generated from gaussian distributions and obtaining the optimal (bayes) solution  under this assumption. When the bayes solution is obtained, one can obtain as many separability hyperplanes as number of classes to classify minus one. The projection of the data over these hyperplanes provides us a dimensionality reduction and, therefore, a feature extraction approach.\n",
    "\n",
    "Use the [LDA()](http://scikit-learn.org/stable/modules/generated/sklearn.lda.LDA.html) method to implement this approach. Remember that, in classification problems, CCA is equivalent to LDA, so you shpuld obtain similar results to those of the previous section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "N_feat_max = n_classes -1 # As many new features as classes minus 1\n",
    "# 1. Obtain LDA or CCA projections\n",
    "cca = LinearDiscriminantAnalysis(n_components=N_feat_max)\n",
    "cca.#<FILL IN>\n",
    "X_train_cca = #<FILL IN>\n",
    "X_val_cca = #<FILL IN>\n",
    "X_test_cca = #<FILL IN>\n",
    "\n",
    "# 2. Compute and plot accuracy evolution\n",
    "rang_feat = np.arange(1, N_feat_max, 1)\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(X_train_cca, Y_train, X_val_cca, Y_val, X_test_cca, Y_test, rang_feat)\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# 3. Find the optimum number of features\n",
    "pos_max = #<FILL IN>\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat, 13, 'incorrect result: number of optimum dimensions is uncorrect')\n",
    "Test.assertEquals(np.round(test_acc_opt,4), 0.9000, 'incorrect result: test accuracy is uncorrect')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
