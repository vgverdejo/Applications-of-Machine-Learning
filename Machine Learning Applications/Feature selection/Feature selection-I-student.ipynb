{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab Session : Feature selection I**\n",
    "\n",
    "\n",
    "Author: Vanessa GÃ³mez Verdejo (http://vanessa.webs.tsc.uc3m.es/)\n",
    "\n",
    "Updated: 19/04/2017 (working with sklearn 0.18.1)\n",
    "\n",
    "In this lab session we are going to study filtering methods for feature selection. To analyze their performances we will work with the Olivetti face detection problem. \n",
    "\n",
    "#### ** During this lab we will cover: **\n",
    "#### *Part 1: Filter methods for feature selection* \n",
    "##### *   Part 1.1: F-score* \n",
    "##### *   Part 1.2: Mutual Information* \n",
    "##### *   Part 1.3: Random Forest* \n",
    "#### *Part 2: Advanced work:*\n",
    "    1. HSIC \n",
    "    2. MRmr\n",
    "#### *Part 3: More advanced work: search algorithms* \n",
    "\n",
    "To implement the different approaches we will base on [Scikit-Learn](http://scikit-learn.org/stable/) python toolbox.\n",
    "\n",
    "As you progress in this notebook, you will have to complete some exercises. Each exercise includes an explanation of what is expected, followed by code cells where one or several lines will have written down `<FILL IN>`.  The cell that needs to be modified will have `# TODO: Replace <FILL IN> with appropriate code` on its first line.  Once the `<FILL IN>` sections are updated and the code can be run; below this cell, you will find the test cell (beginning with the line `# TEST CELL`) and you can run it to verify the correctness of your solution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 0: Download and prepare the data **\n",
    "\n",
    "Olivetti dataset consists of ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).\n",
    "    \n",
    "The next code includes the lines to download this data set and create the training, validation and test data partitions, as well as normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_olivetti_faces \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "print 'The first time that you downlaod the data it can take a while...'\n",
    "olivetti_people = fetch_olivetti_faces()\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = olivetti_people.images.shape\n",
    "\n",
    "# for machine learning we use the data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = olivetti_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "Y = olivetti_people.target\n",
    "n_classes = np.unique(Y).shape[0]\n",
    "\n",
    "print(\"Dataset size information:\")\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Preparing the data\n",
    "\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# split into a training and validation set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.333)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Binarize the labels for some feature selection methods\n",
    "set_classes = np.unique(Y)\n",
    "Y_train_bin = label_binarize(Y_train, classes=set_classes)\n",
    "\n",
    "print(\"Number of training samples: %d\" % X_train.shape[0])\n",
    "print(\"Number of validation samples: %d\" % X_val.shape[0])\n",
    "print(\"Number of test samples: %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 1: Filtering methods **\n",
    "\n",
    "Filtering methods are characterized for being independent from the subsequent classification stage, so they use a relevance criterion to measure the discriminatory capability of each feature and select or rank the input features according this criterion.\n",
    "\n",
    "Here, we are going to study two possible approaches: \n",
    "* F-score (the F-test reduces to two-sample T-test when we work with binary problems): which uses a statistical test to evaluate whether the data of the different classes have been generated by different distributions or not. In this case, this criterion evaluates the relevance independently for each feature, so it is said to be univariate.\n",
    "* Random Forest: in this case we can train a random forest and analyze the number of times that a feature have been used in the forest. More used features will be more relevant. In this case the relevance of a feature is analyzed in combination with the remaining ones (each tree of the forest use several features and the fact that one is selected depends on the those selected previously), so this feature selection criterion is call multivariate.\n",
    "\n",
    "After selecting the subset of relevant features, we will analyze their discriminatory capability using a linear SVM as classifier and use its final test accuracy to evaluate the goodness of the different selection methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1.1: F-score **\n",
    "\n",
    "Use the [f_classif()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html) method to obtain the F-score of each input feature (each pixel of the face image). Use the resulting values to rank the features by relevance (starting by the most relevant) and provide in ind\\_rel\\_feat the position of the sorted features (i.e. ind_rel_feat[0] has contain the position of the most relevant feature). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# F-score\n",
    "from sklearn.feature_selection import f_classif\n",
    "F, p = f_classif(#<FILL IN>)  # Returns F-score and their associated p values\n",
    "\n",
    "# sort in descending order\n",
    "ind_rel_feat = # <FILL IN>\n",
    "    \n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, ind_rel_feat[f], F[ind_rel_feat[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.sum(ind_rel_feat[:5]- [ 19, 148,  18,  17, 150]),0, 'incorrect result: first positions of relevant features are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to provide you two functions:\n",
    "* SVM_accuracy_evolution( ) which analyzes the accuracy of a linear SVM when different number of features are used, specifically, those given by the input variable rang\\_feat. Besides, it computes the training, validation and test accuracy, so we can later use these values to select the optimum number features and select the subset of relevant ones.\n",
    "* plot_accuracy_evolution( ) which directly let you plot the results provided by  SVM_accuracy_evolution( ).\n",
    "\n",
    "Note that you must provide to SVM_accuracy_evolution() the data with the variables sorted by relevance, so that the most relevant ones are used first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def SVM_accuracy_evolution(X_train_s, Y_train, X_val_s, Y_val, X_test_s, Y_test, rang_feat):\n",
    "    \"\"\"Compute the accuracy of training, validation and test data for different the number of features given\n",
    "        in rang_feat.\n",
    "\n",
    "    Args:\n",
    "        X_train_s (numpy dnarray): training data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_train (numpy dnarray): labels of the training data (number data x 1).\n",
    "        X_val_s (numpy dnarray): validation data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_val (numpy dnarray): labels of the validation data (number data x 1).\n",
    "        X_test_s (numpy dnarray): test data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_test (numpy dnarray): labels of the test data (number data x 1).\n",
    "        rang_feat: range with different number of features to be evaluated                                           \n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the model to train a liner SVM and adjust by CV the parameter C\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    acc_tr = []\n",
    "    acc_val = []\n",
    "    acc_test = []\n",
    "    for i in rang_feat:\n",
    "        # Train SVM classifier\n",
    "        clf.fit(X_train_s[:, :i], Y_train)\n",
    "        # Compute accuracies\n",
    "        acc_tr.append(clf.score(X_train_s[:, :i], Y_train))\n",
    "        acc_val.append(clf.score(X_val_s[:, :i], Y_val))\n",
    "        acc_test.append(clf.score(X_test_s[:, :i], Y_test))\n",
    "\n",
    "    return np.array(acc_tr), np.array(acc_val), np.array(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test):\n",
    "\n",
    "    \"\"\"Plot the accuracy evolution for training, validation and test data sets.\n",
    "    Args:\n",
    "        rang_feat: range with different number of features where the accuracy has been evaluated   \n",
    "        acc_tr: numpy vector with the training accuracies\n",
    "        acc_val: numpy vector with the validation accuracies\n",
    "        acc_test: numpy vector with the test accuracies                                          \n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(rang_feat, acc_tr, \"b\", label=\"train\")\n",
    "    plt.plot(rang_feat, acc_val, \"g\", label=\"validation\")\n",
    "    plt.plot(rang_feat, acc_test, \"r\", label=\"test\")\n",
    "    plt.xlabel(\"Number of features\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title('Accuracy evolution')\n",
    "    plt.legend(['Training', 'Validation', 'Test'], loc = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code lines to properly call to SVM\\_accuracy\\_evolution( ) function. Then, use the returned accuracies to validate the number of features to use and obtain its corresponding test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(5, 1000, 10) \n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat,465, 'incorrect result: number of selected features is uncorrect')\n",
    "Test.assertEquals(test_acc_opt,0.85, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, next function, plot_image( ), let you plot the image of any face of the data set. However, we can use it to plot the relevance of each pixel or just a mask indicating the selected input features. Next cell contains an example to plot the F-score values, complete the following code to plot a mask of the selected features after the validation process (i.e., create an image with 1 in the positions of the selected features and 0 in the positions of the non selected ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_image(image, h, w):\n",
    "    \"\"\"Helper function to plot a face image\n",
    "    Args:\n",
    "        image: numpy vector with the image to plot (of dimensions h*w)\n",
    "        h: height of the image (in number of pixels)\n",
    "        w: width of the image (in number of pixels)  \"\"\"  \n",
    "    \n",
    "    plt.imshow(image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example to plot an image\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_image(F, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Create the mask of selected features\n",
    "# Several lines to <FILL IN>\n",
    "\n",
    "mask = # <FILL IN>\n",
    "\n",
    "# Plot it!\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_image(mask, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1.2: Mutual Information **\n",
    "\n",
    "\n",
    "Compute the MI between each input feature and the output labes to measure the relevance of each feature. Check the function [mutual_info_classif()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif).\n",
    "\n",
    "Note: this MI estimator has a parameter (n_neighbors) which should be validated. You can use its default value for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Obtain MI values\n",
    "MI = mutual_info_classif(#<FILL IN>, random_state =0)  # Returns MI values\n",
    "\n",
    "# sort in descending order\n",
    "ind_rel_feat = # <FILL IN>\n",
    "    \n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, ind_rel_feat[f], MI[ind_rel_feat[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.sum(ind_rel_feat[:5]- [ 86, 150, 147,  83,  85]),0, 'incorrect result: first positions of relevant features are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the above the result to:\n",
    "* Evaluate the linear SVM performance with the most relevant features are used (use SVM_accuracy_evolution( ) function). Plot the evolution of the training, validation and test accuracies.\n",
    "* Obtain, using the validation accuracy, the optimum number of features to use.\n",
    "* Create the mask with the selected features and plot it (you can use the plot_image( ) function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(5, 1000, 10) \n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))\n",
    "    \n",
    "# Create and plot the mask\n",
    "mask = # <FILL IN>\n",
    "plt.figure()\n",
    "plot_image(mask, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat,555, 'incorrect result: number of selected features is uncorrect')\n",
    "Test.assertEquals(test_acc_opt,0.85, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1.3: Random Forest **\n",
    "\n",
    "Train a random forest to obtain a measurement of the relevance of each feature. You can use the  [RandomForestClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) function, which returns the feature relevances in the variable .feature\\_importances\\_.\n",
    "\n",
    "\n",
    "Complete the following code to:\n",
    "* Train a random forest classifier with the parameters given by default and 250 trees.\n",
    "* Use the forest feature importances to obtain a ranking with the most relevant features, save their positions in the variable  ind\\_rel\\_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(1)\n",
    "# Build a forest and obtain the feature importances\n",
    "forest = RandomForestClassifier(n_estimators=250)\n",
    "forest.fit(X_train, Y_train)\n",
    "importances = # <FILL IN>\n",
    "\n",
    "# Obtain the positions of the sorted features (the most relevant first)\n",
    "ind_rel_feat = # <FILL IN>\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, ind_rel_feat[f], importances[ind_rel_feat[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.sum(ind_rel_feat[:5]-  [ 90, 598,  527, 150, 684]),0, 'incorrect result: first positions of relevant features are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the above the result to:\n",
    "* Evaluate the linear SVM performance with the most relevant features are used (use SVM_accuracy_evolution( ) function). Plot the evolution of the training, validation and test accuracies.\n",
    "* Obtain, using the validation accuracy, the optimum number of features to use.\n",
    "* Create the mask with the selected features and plot it (you can use the plot_image( ) function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(5, 1000, 10) # To speed up the execution, we use steps of 10 features.\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = # <FILL IN>\n",
    "test_acc_opt = # <FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))\n",
    "\n",
    "# Create and plot the mask\n",
    "mask = # <FILL IN>\n",
    "plt.figure()\n",
    "plot_image(mask, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat,955, 'incorrect result: number of selected features is uncorrect')\n",
    "Test.assertEquals(test_acc_opt,0.91, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 2: Advanced work **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to implement additional feature selection approaches which are not included in scikit-learn library.\n",
    "\n",
    "In particular, we propose to implement 2 different schemes:\n",
    "1. HSIC relevance measurement\n",
    "2. MRmr (Maximum Relevance minimum redundance) approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. HSIC relevance measurement\n",
    "\n",
    "Here, you have two usefull functions:\n",
    "1. estimate_gamma( ), which let you obtain a good estimation of the gamma value to be used by the kernel transformations. In this way, you can avoid validating this parameter.\n",
    "2. center_K( ), which removes the mean of a kernel matrix in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_gamma(X):\n",
    "    \"\"\"Estimate an appropiate valie of the gamma parameter to be used the build a RBF kernel with the data X\n",
    "    Args:\n",
    "        X: input data                             \n",
    "    \"\"\"\n",
    "    size=X.shape[0];\n",
    "    if X.ndim==1:\n",
    "        X=X[:,np.newaxis]\n",
    "        \n",
    "    G=(X* X).sum(axis=1)\n",
    "    KK=np.dot(X,X.T)\n",
    "    \n",
    "    R = np.tile(G, [size,1])\n",
    "    Q = R.T\n",
    "    \n",
    "    dist=(Q + R - 2*KK)\n",
    "   \n",
    "    aux=dist-np.tril(dist)\n",
    "    aux=aux.reshape(size**2,1)\n",
    "    sig=np.sqrt(0.5*np.mean(aux[np.where(aux>0)]))  \n",
    "    gamma = 1./sig**2  \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_K(K):\n",
    "    \"\"\"Center a kernel matrix K, i.e., removes the data mean in the feature space.\n",
    "\n",
    "    Args:\n",
    "        K: kernel matrix                                        \n",
    "    \"\"\"\n",
    "    size_1,size_2 = K.shape;\n",
    "    D1 = K.sum(axis=0)/size_1 \n",
    "    D2 = K.sum(axis=1)/size_2\n",
    "    E = D2.sum(axis=0)/size_1\n",
    "\n",
    "    K_n = K + np.tile(E,[size_1,size_2]) - np.tile(D1,[size_1,1]) - np.tile(D2,[size_2,1]).T\n",
    "    return K_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, considering that the HSIC value between a input feature $\\textbf{X}$ and an output variable $\\textbf{Y}$ with length $N$ is given by:\n",
    "$$ \\text{HSIC}(\\textbf{X},\\textbf{Y}) = \\dfrac{1}{N^2} \\text{Tr}(\\tilde{K}_x \\tilde{K}_y)$$\n",
    "where $\\tilde{K}_x$ and $\\tilde{K}_y$ are the centered kernel matrices of the corresponding input variables $\\textbf{X}$ and $\\textbf{Y}$, complete the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def HSIC_rbf(X, Y):\n",
    "    \"\"\"Compute HSIC value between input and output data using a RBF kernel\n",
    "\n",
    "    Args:\n",
    "        X: input data\n",
    "        Y: output data\n",
    "    \"\"\"\n",
    "    if X.ndim==1:\n",
    "        X=X[:,np.newaxis]\n",
    "    if Y.ndim==1:\n",
    "        Y=Y[:,np.newaxis]\n",
    "    # 1. Estimate gamma value for X and Y\n",
    "    gamma_x = # <FILL IN>\n",
    "    gamma_y = # <FILL IN>\n",
    "    \n",
    "    # 2. Compute kernel matrices\n",
    "    K_x = # <FILL IN>\n",
    "    K_y = # <FILL IN>\n",
    "    \n",
    "    # 3. Center kernel matrices\n",
    "    K_xc = # <FILL IN>\n",
    "    K_yc = # <FILL IN>\n",
    "    \n",
    "    # 4. Compute HSIC value\n",
    "    HSIC= # <FILL IN>\n",
    "    return HSIC\n",
    "\n",
    "# Test HSIC function\n",
    "i_test = 90\n",
    "HSIC=HSIC_rbf(X_train[:,i_test], Y_train_bin)\n",
    "print HSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.round(HSIC,4), 0.0083, 'incorrect result: HSIC funtion does not work properly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Compute HSIC relevances\n",
    "importances=np.zeros(X_train.shape[1])\n",
    "for i in range(X_train.shape[1]):\n",
    "    importances[i]=# <FILL IN>\n",
    "    \n",
    "# Obtain the positions of the sorted features (the most relevant first)\n",
    "ind_rel_feat = # <FILL IN>\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, ind_rel_feat[f], importances[ind_rel_feat[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.sum(ind_rel_feat[:5]-  [ 1674, 1555,  1616,  1554, 1675]),0, 'incorrect result: first positions of relevant features are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the above the result to:\n",
    "* Evaluate the linear SVM performance with the most relevant features are used (use SVM_accuracy_evolution( ) function). Plot the evolution of the training, validation and test accuracies.\n",
    "* Obtain, using the validation accuracy, the optimum number of features to use.\n",
    "* Create the mask with the selected features and plot it (you can use the plot_image( ) function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(5, 1000, 10) # To speed up the execution, we use steps of 10 features.\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = # <FILL IN>\n",
    "test_acc_opt = # <FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))\n",
    "\n",
    "# Create and plot the mask\n",
    "mask = # <FILL IN>\n",
    "plt.figure()\n",
    "plot_image(mask, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat,395, 'incorrect result: number of selected features is uncorrect')\n",
    "Test.assertEquals(test_acc_opt,0.83, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. MRmr\n",
    "\n",
    "In this section let's implement a Maximum Relevance minimum redundance (MRmr) approach to use univariate relevance measurements in a multivariate fashion. In particular, let's use:\n",
    "1. **f_classif** as relevance criterion.\n",
    "2. **correlation** as redundancy measurement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Variable initialization\n",
    "n_var = X_train.shape[1]\n",
    "var_sel = np.empty(0,dtype=int) # subset of selected features\n",
    "var_cand = np.arange(n_var) # subset of candidate features\n",
    "\n",
    "# Precompute relevances (f_classif value of all input variables wiht output variable)\n",
    "relevances,p = #<FILL IN>\n",
    "\n",
    "# Precomupute redundancies (correlation among all input variables, it is a matrix of n_var x n_var)\n",
    "redundancies = #<FILL IN>\n",
    "\n",
    "# Select the most relevant feature (the one with maximum relevance)\n",
    "sel = #<FILL IN>\n",
    "# Add it to the subset of selected features\n",
    "var_sel = #<FILL IN>\n",
    "# Remove it from the subset of candidate features\n",
    "var_cand = #<FILL IN>\n",
    "\n",
    "# Iteratively select variables\n",
    "for i in range(n_var-1):\n",
    "    # Get relevance values of the var_cand variables\n",
    "    relevances_cand=#<FILL IN>\n",
    "    \n",
    "    # Compute redundancies with selected features:\n",
    "    # from the redundancies matrix select the rows of var_sel and the columns of var_cand\n",
    "    redundancy_sel = #<FILL IN>   \n",
    "    # Average the redundancy values over the selected features (rows) \n",
    "    # to get a redundancy value for each candidate variables   \n",
    "    redundancy_cand=#<FILL IN>\n",
    "    \n",
    "    # Compute MRmr = relevances_cand - redundancy_cand\n",
    "    MRmr=#<FILL IN>\n",
    "    \n",
    "    # Select the new feature as the one with the maximum MRmr value\n",
    "    sel=#<FILL IN>\n",
    "    # Add it to the subset of selected features\n",
    "    var_sel=#<FILL IN>\n",
    "    # Remove it from the subset of candidate features\n",
    "    var_cand= #<FILL IN>\n",
    "\n",
    "ind_rel_feat = var_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.sum(ind_rel_feat[:5]-  [19, 148, 18, 17, 150]),0, 'incorrect result: first positions of relevant features are uncorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(5, 1000, 10) # To speed up the execution, we use steps of 10 features.\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = # <FILL IN>\n",
    "test_acc_opt = # <FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))\n",
    "\n",
    "# Create and plot the mask\n",
    "mask = # <FILL IN>\n",
    "plt.figure()\n",
    "plot_image(mask, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat,435, 'incorrect result: number of selected features is uncorrect')\n",
    "Test.assertEquals(test_acc_opt,0.85, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: More advanced work **\n",
    "\n",
    "Implement a search approach (either fordward or backward) to combine with any relevance criteria to be able to find the best subset of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
