{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab Session : Feature selection II**\n",
    "\n",
    "Here, we will continue with work of the notebook Feature Selection I, and we will focus on wrapper and embedded feature selection methods. To analyze their performances we will continue working with the Olivetti face detection problem. \n",
    "\n",
    "#### ** During this lab we will cover: **\n",
    "#### *Part 1: Wrapers for feature selection: RFE* \n",
    "#### *Part 2: Embedded methods: L1 regularization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 0.1: Download and prepare the data **\n",
    "\n",
    "Olivetti dataset consists of ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).\n",
    "    \n",
    "The next code includes the lines to download this data set and create the training, validation and test data partitions, as well as normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time that you downlaod the data it can take a while...\n",
      "Dataset size information:\n",
      "n_features: 4096\n",
      "n_classes: 40\n",
      "Number of training samples: 200\n",
      "Number of validation samples: 100\n",
      "Number of test samples: 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import fetch_olivetti_faces \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "print 'The first time that you downlaod the data it can take a while...'\n",
    "olivetti_people = fetch_olivetti_faces()\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = olivetti_people.images.shape\n",
    "\n",
    "# for machine learning we use the data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = olivetti_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "Y = olivetti_people.target\n",
    "n_classes = np.unique(Y).shape[0]\n",
    "\n",
    "print(\"Dataset size information:\")\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Preparing the data\n",
    "\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# split into a training and validation set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.333)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Binarize the labels for some feature selection methods\n",
    "set_classes = np.unique(Y)\n",
    "Y_train_bin = label_binarize(Y_train, classes=set_classes)\n",
    "\n",
    "print(\"Number of training samples: %d\" % X_train.shape[0])\n",
    "print(\"Number of validation samples: %d\" % X_val.shape[0])\n",
    "print(\"Number of test samples: %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 0.2: Some utilities **\n",
    "\n",
    "Here, you can find some functions from the notebook Feature Selection I which can be usefull for completing the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def SVM_accuracy_evolution(X_train_s, Y_train, X_val_s, Y_val, X_test_s, Y_test, rang_feat):\n",
    "    \"\"\"Compute the accuracy of training, validation and test data for different the number of features given\n",
    "        in rang_feat.\n",
    "\n",
    "    Args:\n",
    "        X_train_s (numpy dnarray): training data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_train (numpy dnarray): labels of the training data (number data x 1).\n",
    "        X_val_s (numpy dnarray): validation data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_val (numpy dnarray): labels of the validation data (number data x 1).\n",
    "        X_test_s (numpy dnarray): test data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_test (numpy dnarray): labels of the test data (number data x 1).\n",
    "        rang_feat: range with different number of features to be evaluated                                           \n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the model to train a liner SVM and adjust by CV the parameter C\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    acc_tr = []\n",
    "    acc_val = []\n",
    "    acc_test = []\n",
    "    for i in rang_feat:\n",
    "        # Train SVM classifier\n",
    "        clf.fit(X_train_s[:, :i], Y_train)\n",
    "        # Compute accuracies\n",
    "        acc_tr.append(clf.score(X_train_s[:, :i], Y_train))\n",
    "        acc_val.append(clf.score(X_val_s[:, :i], Y_val))\n",
    "        acc_test.append(clf.score(X_test_s[:, :i], Y_test))\n",
    "\n",
    "    return np.array(acc_tr), np.array(acc_val), np.array(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test):\n",
    "\n",
    "    \"\"\"Plot the accuracy evolution for training, validation and test data sets.\n",
    "    Args:\n",
    "        rang_feat: range with different number of features where the accuracy has been evaluated   \n",
    "        acc_tr: numpy vector with the training accuracies\n",
    "        acc_val: numpy vector with the validation accuracies\n",
    "        acc_test: numpy vector with the test accuracies                                          \n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(rang_feat, acc_tr, \"b\", label=\"train\")\n",
    "    plt.plot(rang_feat, acc_val, \"g\", label=\"validation\")\n",
    "    plt.plot(rang_feat, acc_test, \"r\", label=\"test\")\n",
    "    plt.xlabel(\"Number of features\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title('Accuracy evolution')\n",
    "    plt.legend(['Training', 'Validation', 'Test'], loc = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_image(image, h, w):\n",
    "    \"\"\"Helper function to plot a face image\n",
    "    Args:\n",
    "        image: numpy vector with the image to plot (of dimensions h*w)\n",
    "        h: height of the image (in number of pixels)\n",
    "        w: width of the image (in number of pixels)  \"\"\"  \n",
    "    \n",
    "    plt.imshow(image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### *Part 1: Wrapers for feature selection: RFE* \n",
    "\n",
    "The Recursive Feature Elimination (RFE) method iteratively trains a set of SVM classifier and, in each step, it eliminates a feature (or a subset of features) in such a way that the classification margin is reduced the least.\n",
    "\n",
    "Scikit-Learn provides a function with a full implementation of the RFE method. This function, [RFE( )](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html), let user select the classifier to consider, the number of selected features (parameter n_features_to_select) and the number of features removed in each step (parameter step).  As result, in parameter .ranking returns the ranking position of the each feature (i.e., .ranking\\_[i] corresponds to the ranking position of the i-th feature).\n",
    "\n",
    "Complete the following code to:\n",
    "* Train the RFE approach with a linear SVM fixing  n_features_to_select to 1 (so that we run the RFE method until the end obtaining a full raking with all the features) and step to 10 (just to speed up the training).\n",
    "* Use the content of .ranking\\_ to obtain a ranking with the most relevant features, save their positions in the variable  ind\\_rel\\_feat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print 'The training of this method can take some minutes, be patient...'\n",
    "# Define the classifier, the RFE method and train it\n",
    "estimator = SVC(kernel=\"linear\")\n",
    "RFE_selector = RFE(#<FILL IN>)\n",
    "RFE_selector.fit(X_train, Y_train)\n",
    "\n",
    "# Obtain the positions of the sorted features (the most relevant first)\n",
    "ind_rel_feat = # <FILL IN>\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d\" % (f + 1, ind_rel_feat[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.sum(ind_rel_feat[:5]- [781, 3615, 648,  1016, 51]),0, 'incorrect result: first positions of relevant features are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the random forest section, use the above the result to:\n",
    "* Evaluate the linear SVM performance with the most relevant features are used (use SVM_accuracy_evolution( ) function). Plot the evolution of the training, validation and test accuracies.\n",
    "* Obtain, using the validation accuracy, the optimum number of features to use.\n",
    "* Create the mask with the selected features and plot it (you can use the plot_image( ) function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(5, 1000, 10) # To speed up the execution, we use steps of 10 features.\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = # <FILL IN>\n",
    "test_acc_opt = # <FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_opt_feat)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))\n",
    "\n",
    "# Create and plot the mask\n",
    "mask = # <FILL IN>\n",
    "plt.figure()\n",
    "plot_image(mask, h, w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(num_opt_feat,395, 'incorrect result: number of selected features is uncorrect')\n",
    "Test.assertEquals(test_acc_opt,0.88, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 2: Embedded methods:  L1 regularization**\n",
    "\n",
    "Embedded methods are able to carry out the feature selection process during the classifier training, so both stages (feature selection and classifier training) are completely linked and, therefore, the selection process is guided by the classifier.\n",
    "\n",
    "In this case, we are going to study two well-known techniques:  \n",
    "* L1 SVM: in this case we can train linear SVM regularized with a L1 penalty which is able to provide sparsity over the weights vector. As to obtain the classifier output we have to multiply the input data by this vector, those input features associated to the zeros of the weight vector are not used during the classification process (at least, in binary problem, we later analyze this in detail for the multiclass case). \n",
    "\n",
    "* L1 Logistic Regression: here, we will apply the above L1 penalty, but using a Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2.1: L1-SVM**\n",
    "\n",
    "In this last section, we are going to analyze the properties of the L1-SVM as feature selection approach. For this purpose, we are going to use the linear SVM implementation given by the method LinearSVC, which directly let us select the regularization to be used.\n",
    "\n",
    "Unlike previous methods, here we cannot obtain a ranking of variables. In this case, we have to sweep the value of the regularization parameter in order to get a higher (or lower) sparsity of the weight vector. According to this, complete the following code to:\n",
    "* train a linear l1-SVM for different values of the regularization parameter, for each value obtain the training, validation and test accuracies, as well as the sparsity rate. \n",
    "* Plot the resulting accuracy curves.\n",
    "* Finally, analyzing the validation accuracy, select the optimum value of C.\n",
    "\n",
    "Note: Sparsity Rate (SR) is defined as the mean number of zeros of the SVM weight vector (check the parameter .coef\\_ of the SVM classifier), then, if all elements are zero, SR has to be 100%. In our case, that we are working with a multiclass problem, we have a set of vectors, so we have to average the number of zeros overall the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.svm import LinearSVC\n",
    "np.random.seed(1)\n",
    "# Defining some useful variables to save results\n",
    "acc_tr = []\n",
    "acc_val = []\n",
    "acc_test = []\n",
    "sparsity_rate = []\n",
    "\n",
    "# Defining the range of C values to explore\n",
    "rang_C = 2*np.logspace(-2, 2, 20)\n",
    "\n",
    "print 'The training of this method can take some minutes, be patient...'\n",
    "for i, C in enumerate(rang_C):\n",
    "    # Define and train SVM classifier\n",
    "    svm_l1 = LinearSVC(C=C, penalty=\"l1\", dual=False)\n",
    "    svm_l1.fit(X_train, Y_train)\n",
    "    \n",
    "    # Compute the sparsity rate (coef_l1 contains zeros due to the\n",
    "    # L1 sparsity inducing norm)\n",
    "    \n",
    "    sparsity_rate.append(# <FILL IN>)\n",
    "    \n",
    "    # Compute accuracies\n",
    "    acc_tr.append(#<FILL IN>)\n",
    "    acc_val.append(#<FILL IN>)\n",
    "    acc_test.append(#<FILL IN>)\n",
    "\n",
    "    \n",
    "# Plot the accuracy curves\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(sparsity_rate, acc_tr, acc_val, acc_test)\n",
    "plt.xlabel(\"Sparsity rate\")\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum value of C\n",
    "C_opt = # <FILL IN>\n",
    "test_acc_opt = # <FILL IN>\n",
    "\n",
    "print 'Optimum value of C: ' + str(C_opt)\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.round(C_opt,2), 2.07, 'incorrect result: the selected value of C is uncorrect')\n",
    "Test.assertEquals(test_acc_opt,0.9, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, the L1-SVM has provided a sparsity vector instead of a real feature selection. Note that, as we are working in a multiclass problem, we have as many vectors as categories (we are training a 1 vs. rest classifier) and each vector has the zeros in different positions, so each classifier is using different input features. So, we obtain a feature selection per categories.\n",
    "\n",
    "To analyze this selection per categories, you can run the following code which will train again the L1-SVM with optimum value of the C that we have just selected. Then, it will let you get the set of weight vectors and generate a set of masks with the selected variables for each class. Finally, plot all these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the linear SVM with the optimum value of C\n",
    "svm_l1 = LinearSVC(C=C_opt, penalty=\"l1\", dual=False)\n",
    "svm_l1.fit(X_train, Y_train)\n",
    "coef_l1 = svm_l1.coef_\n",
    "coef_l1.shape\n",
    "\n",
    "#### Useful function for display purposes ##########\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_gallery(images, titles, h, w, n_row=4, n_col=10):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "##################################################### \n",
    "\n",
    "# Create a mask per class\n",
    "mask_class = np.zeros(coef_l1.shape)\n",
    "mask_class [np.where(coef_l1>0)] = 1\n",
    "\n",
    "# Plot all the masks\n",
    "titles = ['class '+str(c) for c in set_classes]\n",
    "plot_gallery(mask_class, titles, h, w)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can convert this feature selection per class in a real feature selection. For this purpose, we just have to select all the input features which are used in at least one individual classifier. \n",
    "Compete the following code to be able to:\n",
    "* Find the selected features (those used by at least one individual classifier). \n",
    "* Save in num\\_var\\_sel the number of selected features.\n",
    "* Train a linear SVM with these features and compute its test accuracy.\n",
    "* Plot the mask of selected features\n",
    "\n",
    "If we define the feature relevance as the number of classifiers that use that feature, could you obtain a mask of feature relevances?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn import svm\n",
    "\n",
    "# Finding the selected fatures\n",
    "ind_var_sel = # <FILL IN>\n",
    "\n",
    "# Save the number of selected features\n",
    "num_var_sel = # <FILL IN>\n",
    "\n",
    "# Evaluating performance (for comparison purposes, let's use SVC classifier)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(# <FILL IN>)\n",
    "acc_test = # <FILL IN>\n",
    "\n",
    "print 'Number optimum of features: ' + str(num_var_sel )\n",
    "print(\"The test accuracy with real feature selection is  %2.2f%%\" %(100*acc_test))\n",
    "\n",
    "# Obtain and plot the mask\n",
    "mask = # <FILL IN>\n",
    "plot_image(mask, h, w)\n",
    "plt.show()\n",
    "    \n",
    "#Plot a relevance mask\n",
    "# <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Part 2.2: L1-Logistic Regression**\n",
    "\n",
    "The Scikit-Learn Logistic Regression implementation lets us add an L1 regularization to obtain sparsity over the weigths vector, see help\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "So, using the L1-SVM code as re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
